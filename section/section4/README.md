# 4. 카프카 상세 개념 설명
목차

- 토픽과 파티션
- 카프카 프로듀서
- 카프카 컨슈머
- 스프링 카프카
- 정리

## 토픽과 파티션

### 적정 파티션 개수

토픽의 파티션 개수는 카프카의 성능과 관련이 있으므로 적절한 파티션 개수를 설정하고 운영하는 것이 중요하다.

토픽 생성시 파티션의 개수를 정하는 데에 고려해야 할 점은 3가지 이다.

- 데이터 처리량
- 메시지 키 사용 여부
- 브로커, 컨슈머 영향도
    - 카프카는 파일시스템을 사용하므로 파티션의 수 만큼 파일을 접근한다고 보면 된다.</br>
      이는 운영체제 특성상 프로세스당 열 수 있는 파일 최대 개수가 있으므로 브로커가 접근하는 파일 개수를 안정적으로 유지하기 위해서는 카프카 브로커를 늘려야 한다.</br>

데이터 처리 속도를 올리는 방법은 2가지 이다.

1. 컨슈머의 처리량을 늘리는 방법
    - 컨슈머의 처리량을 늘리기 위해서는 컨슈머가 실행되는 서버를 스케일 업하거나 GC 튜닝 등 하는 것이다.</br>
      하지만 컨슈머 특성상 다른 시스템(S3, 하둡, 오라클 등)과 연동되기 때문에 일정 수준 이상 처리량을 올리기가 힘들다.</br>
2. 컨슈머를 추가하여 병렬처리량을 늘리는 방법 (해당 방법이 확실한 방법)
    - 파티션 개수를 늘려 파티션 개수 만큼 컨슈머를 추가</br>
      Ex. 프로듀서가 보내는 데이터가 초당 1,000 레코드 이고 컨슈머가 처리할 수 있는 데이터가 초당 100 레코드라면 최소한으로 필요한 파티션의 개수는 10개이다.</br></br>
      파티션 개수만 무조건 늘리는 것은 좋지 않을 수 있다.</br>
      이유는 파티션 개수를 늘림에 따라 컨슈머와 브로커의 부담이 커질 수 있기 때문이다. 이 또한 고려하여야 한다.

### 토픽 정리 정책(cleanup.policy)

**토픽 삭제 정책(delete policy)**

토픽의 데이터를 삭제할 때는 세그먼트 단위로 삭제를 진행한다.</br>
세그먼트는 토픽의 데이터를 저장하는 명시적인 파일 시스템 단위이다. 그리고 파티션마다 별개로 생성되며 세그먼트의 파일 이름은 오프셋 중 가장 작은 값이 된다.</br>
세그먼트는 여러 조각으로 나뉘는데 segment.bytes 옵션으로 1개의 세그먼트 크기를 설정할 수 있다.</br>
segment.bytes 크기보다 커질 경우에는 기존에 적재하던 세그먼트 파일을 닫고 새로운 새그먼트를 열어서 데이터를 저장한다.</br>

삭제 정책이 실행되는 시점은 시간 또는 용량이 기준이 된다.</br>
retention.ms 는 토픽의 데이터를 유지하는 기간을 밀리초로 설정할 수 있다.</br>
카프카는 일정 주기마다 세그먼트 파일의 마지막 수정 시간과 retention.ms 를 비교하는데, 세그먼트 파일의 마지막 수정 시간이 retention.ms 를 넘어가면 세그먼트는 삭제된다.</br>
그리고 retention.bytes 는 토픽의 최대 데이터 크기를 제어한다.</br>
retention.bytes 를 넘어간 세그먼트 파일들은 삭제된다.</br>
삭제된 데이터는 복구할 수 없다.</br>

**토픽 압축 정책(compact policy)**

토픽 압축 정책이란 메시지 키별로 해당 메시지 키의 레코드 중 오래된 데이터를 삭제하는 정책이다.</br>
압축 정책은 액티브 세그먼트를 제외한 나머지 세그먼트들에 한해서만 데이터를 처리한다.</br>
데이터의 압축 시작 시점은 min.cleanable.clity.ratio 옵션값을 따른다.</br>
min.cleanable.clity.ratio 옵션값은 액티브 세그먼트를 제외한 세그먼트에 남아 있는 데이터의 tail 영역의 레코드 개수와 head 영역의 레코드 개수의 비율을 뜻한다.</br>

<img src="/img/4.1.2-3.png" width="1000px;">

tail 영역의 레코드들은 ‘clean log’ 라 부르고 압축이 완료됐기 때문에 tail 영역에는 중복된 메시지 키가 없다.

head 영역의 레코드들은 ‘dirty log’ 라고 부르고 압축이 되기 전 레코드들이 있으므로 중복된 메시지 키를 가진 레코드들이 있다.

토픽의 압축은 min.cleanable.dirty.ratio 값에 따라 수행되는데, 옵션값을 0.5 로 설정할 경우 dirty log 비율이 0.5 가 넘어가면 압축이 수행된다.

옵션값이 낮을 수록 압축이 더 자주 발생하게되는데 자주 발생하는 만큼 브로커에 부담을 줄 수 었으므로 적절한 값을 설정하는 것이 중요하다.

### ISR(In-Sync-Replicas)

ISR 은 리더 파티션과 팔로워 파티션이 모두 싱크가 된 상태를 뜻한다.
ISR 이라는 용어가 나온 이유는 팔로워 파티션이 리더 파티션으로부터 데이터를 복제하는 데에 시간이 걸리기 때문이다.
리더 파티션에 새로운 레코드가 추가되어 오프셋이 증가하면 팔로워 파티션이 위치한 브로커는 리더 파티션의 데이터를 복제하고 리더 파티션에 데이터가 적재된 이후 팔로워 파티션이 복제하는 시간 차이 때문에 리더 파티션과 팔로워 파티션 간에 오프셋 차이가 발생한다.
이러한 차이를 모니터링 하기 위해 리더 파티션은 replica.lag.time.max.ms 값만큼의 주기를 가지고 팔로워 파티션이 데이터를 복제하는지 확인한다.
만약 팔로워 파티션이 replica.lag.time.max.ms 값보다 더 긴 시간동안 데이터를 가져가지 않는다면 해당 팔로워 파티션에 문제가 생긴것으로 판단하고 ISR 그룹에서 제외하게 된다.

## 카프카 프로듀서

### acks 옵션

카프카 프로듀서의 acks 옵션은 0, 1, all(또는 -1) 값을 가질 수 있다.</br>
이 옵션을 통해 프로듀서가 전송한 데이터가 카프카 클러스터에 얼마나 신뢰성 높게 저장할지 지정할 수 있으며, 옵션에 따라 성능이 달라질 수 있기 때문에 잘알고 써야 한다.</br>

**acks = 0 경우**

프로듀서가 리더 파티션으로 데이터를 전송했을 때 **리더 파티션으로 데이터가 저장되었는지 확인하지 않는다는 뜻**이다.</br>
리더 파티션은 데이터가 저장된 이후에 데이터가 몇 번째 오프셋에 저장되었는지 리턴하게 되는데, acks=0 인 경우에는 리더 파티션에 데이터가 저장되었는지 여부에 대한 응답 값을 받지 않는다.</br>

<img src="/img/4.2.1-1.png" width="1000px;">

프로듀서에는 데이터의 전송이 실패했을 때 재시도를 할 수 있도록 retries 옵션을 설정할 수 있는데, acks 가 0 일 때는 데이터 전송을 성공으로 간주하기 때문에 retries 옵션값은 무의미 하다.</br>
또한, 프로듀서와 브로커 사이의 네트워크 오류나 브로커의 이슈 등으로 인해 데이터가 유실되더라도 프로듀서는 리더 파티션으로부터 응답값을 받지 않기 때문에 지속적으로 다음 데이터를 보낸다. 그렇게 때문에 acks 옵션이 1, all 일때보다 데이터의 전송 속도는 훨씬 빠르다.</br>
즉, 데이터가 일부 유실이 발생하더라도 전송 속도가 중요한 경우에는 이 옵션값(acks=0) 을 사용하면 적절하다.</br>

**acks = 1 경우**

acks  를 1로 설정할 경우 프로듀서는 **보낸 데이터가 리더 파티션에만 정상적으로 적재되었는지 확인**한다.</br>
만약 리더 파티션에 정상적으로 적재되지 않았다면 리더 파티션에 적재될 때까지 재시도할 수 있다.</br>
그러나 리더 파티션에 적재되었음을 보장하더라도 데이터 유실이 발생할 수 있다.</br>
왜냐하면 복제 개수를 2 이상으로 운영할 시 팔로워 파티션이 리더 파티션으로부터 복제하기 직전에 리더 파티션에 있는 브로커에 장애가 발생하면 동기화되지 못한 일부 데이터가 유실될 수 있기 때문이다.</br>

<img src="/img/4.2.1-2.png" width="1000px;">

acks 를 1 로 설정하면 리더 파티션에 데이터가 적재될 때까지 기다린 뒤 응답 값을 받기 때문에 acks 를 0 으로 설정하는 것에 비해 전송 속도가 느리다.

**acks=all 또는 acks=-1 경우**

프로듀서는 보낸 데이터가 리더 파티션과 팔로워 파티션에 모두 정상적으로 적재되었는지 확인한다.</br>
리더, 팔로워 파티션에 적재되었는지를 확인하므로 다른 옵션값보다 데이터 전송 속도가 느리다.</br>
그러나 리더 및 팔로워 파티션에 적재되었지를 기다리기 때문에 일부 브로커에서 장애가 발생하더라도 프로듀서는 안전하게 데이터를 전송하고 저장할 수 있음을 보장할 수 있다.</br>

<img src="/img/4.2.1-3.png" width="1000px;">

acks 를 all 로 설정할 경우에는 토픽 단위로 설정 가능한 min.insync.replicas 옵션값에 따라 데이터의 안정성이 달라진다.</br>
min.insync.replicas 옵션은 프로듀서가 리더 파티션과 팔로워 파티션에 데이터가 적재되었는지 확인하기 위한 최소 ISR그룹의 파티션 개수이다.</br>
예를 들어, min.insync.replicas 의 옵션값이 1이라면 ISR 중 최소 1개 이상의 파티션에 데이터가 적재되었음을 확인하는 것이다.</br>

<img src="/img/4.2.1-4.png" width="1000px;">

min.insync.replicas 의 옵션값을 2이상으로 설정했을 때부터 acks 를 all 로 설정하는 의미가 있다.</br>
이 경우 ISR 의 2개 이상의 파티션에 데이터가 정상 적재되었음을 확인한다는 뜻이다. 이 뜻은 리더 파티션과 팔로워 파티션에 데이터가 정상적으로 적재되었음을 의미한다.</br>

<img src="/img/4.2.1-5.png" width="1000px;">

<img src="/img/4.2.1-6.png" width="1000px;">

상용환경에서는 일반적으로 브로커를 3대 이상으로 묶어 클러스터를 운영한다.</br>
이 점을 고려하여 프로듀서가 데이터를 가장 안정적으로 보내려먼 토픽의 복제 개수는 3, min.insync.replicas 를 2로 설정하고 프로듀서는 acks 를 all 로 설정하는 것을 추천한다.</br>

### 멱등성(idempotence) 프로듀서

멱등성 프로듀서는 동일한 데이터를 여러 번 전송하더라도 카프카 클러스터에 단 한번만 저장되는 것을 의미한다.</br>
기본 프로듀서의 동작 방식은 적어도 한번 전달(at least once delivery) 을 지원한다.</br>
적어도 한번 전달이란 프로듀서가 클러스터에 데이터를 전송하여 저장할 때 적어도 한번 이상 데이터를 적재할 수 있고 데이터가 유실되지 않음을 말하는데, 이는 두번 이상 적재할 가능성이 있으므로 데이터의 중복이 발생할 수 있다.</br>

멱등성 프로듀서를 사용하는 방법은 enable.idempotence 옵션의 값을 true 로 설정하면 된다.</br>
멱등성 프로듀서는 기본 프로듀서와 달리 데이터를 브로커로 전달할 때 프로듀서 PID(Producer unique ID) 와 시퀀스 넘버(sequence number)를 함께 전달한다.</br>
그러면 브로커는 프료듀서의 PID 와 시퀀스 넘버를 확인하여 동일한 메시지의 적재 요청이 오더라도 단 한 번만 데이터를 적재함으로써 프로듀서의 데이터는 정확히 한번 브로커에 적재되도록 동작한다.</br>

<img src="/img/4.2.2-1.png" width="1000px;">

<img src="/img/4.2.2-2.png" width="1000px;">

단, 멱등성 프로듀서는 동일한 세션에서만 정확히 한번 전달을 보장한다.</br>
여기서 말하는 세션은 PID 의 생명주기를 뜻한다.</br>
만약 멱등성 프로듀서로 동작하는 프로듀서 애플리케이션에 문제가 발생하여 종료되고 애플리케이션을 재시작하면 PID 가 달라 진다.</br>
동일한 데이터를 보내더라도 PID 가 달라지면 브로커 입장에서 다른 프로듀서 애플리케이션이 다른 데이터를 보냈다고 판단하기 때문에 멱등성 프로듀서는 장애가 발생하지 않는 경우에만 정확히 한번 적재하는 것을 보장한다는 점을 고려해야 한다.</br>

멱등성 프로듀서를 사용하기 위해 enable.idempotence 를 true 로 설정하면 정확히 한번 적재하는 로직이 성립되기 위해 프로듀서의 일부 옵션들이 강제로 설정된다.</br>
프로듀서의 데이터 재전송 횟수를 정하는 retries 는 기본값으로 Integer.MAX_VALUE 가 되고, acks 옵션은 all 로 설정된다.</br>
이렇게 설정되는 이유는 프로듀서가 적어도 한 번 이상 브로커에 데이터를 보냄으로써 브로커에 단 한 번만 데이터가 적재되는 것을 보장하기 위함이다.</br>

### 트랜잭션(transaction) 프로듀서

카프카 트랜잭션 프로듀서는 다수의 파티션에 데이터를 저장할 경우 모든 데이터에 대해 동일한 원자성(atomic)을 만족시키기 위해 사용된다.</br>
원자성을 만족시킨다는 의미는 다수의 데이터를 동일 트랜잭션으로 묶음으로써 전체 데이터를 처리하거나 전체 데이터를 처리하지 않도록 하는 것이다.</br>
컨슈머는 기본적으로 프로듀서가 보내는 데이터가 파티션에 쌓이는 대로 모두 가져가서 처리한다.</br>
그러나 트랜잭션으로 묶인 데이터를 브로커에서 가져갈 때는 다르게 동작하도록 설정할 수 있다.</br>

트랜잭션 프로듀서를 사용하려면 enable.idempotence 를 true 로 설정하고 transactional.id 를 임의의 String 값으로 정의한다.</br>
그리고 컨슈머의 isolation.level 을 read_committed 로 설정하면 프로듀서와 컨슈머는 트랜잭션으로 처리 완료된 데이터만 쓰고 읽게 된다.</br>

트랜잭션은 파티선의 레코드로 구분한다.</br>
트랜잭션 프로듀서는 사용자가 보낸 데이터를 레코드로 파티션에 저장할 뿐만 아니라 트랜잭션의 시작과 끝을 표현하기 위해 트랜잭션 레코드를 한 개 더 보낸다.</br>
트랜잭션 컨슈머는 파티션에 저장된 트랜잭션 레코드를 보고 트랜잭션이 완료(commit) 되었음을 확인하고 데이터를 가져간다.</br>

<img src="/img/4.2.3-1.png" width="1000px;">

만약 데이터만 존재하고 트랜잭션 레코드가 존재하지 않으면 트랜잭션이 완료되지 않았다고 판단하고 데이터를 가져가지 않는다.
## 카프카 컨슈머

### 멀티 스레드 컨슈머

파티션을 여러 개로 운영하는 경우 데이터를 병렬처리하기 위해서는 파티션 개수와 컨슈머 개수를 동일하게 맞추는 것이 가장 좋은 방법이다.</br>
토픽의 파티션은 1개 이상으로 이루어져 있으며 1개의 파티션은 1개 컨슈머가 할당되어 데이터를 처리할 수 있다.</br>
파티션 개수가 n개라면 동일 컨슈머 그룹으로 묶인 컨슈머 스레드를 최대 n개 운영할 수 있다.</br>
그러므로 n개의 스레드를 가진 1개의 프로세스를 운영하거나 1개의 스레드를 가진 프로세스를 n개 운영하는 방법도 있다.</br>

<img src="/img/4.3.1-1.png" width="1000px;">

개발자의 선택에따라 운영방식을 선정하면 된다.</br>
카프카에서는 공식적으로 지원하는 라이브러리인 자바 멀티 스레드를 지원하므로 자바 애플리케이션에서는 멀티 스레드로 동작하는 멀티 컨슈머 스레드를 개발, 적용할 수 있다.</br>

멀티 스레드로 컨슈머를 안전하게 운영하기 위해서는 고려해야 하는 부분이 많다.

- 하나의 컨슈머 스레드에서 예외적 상황(OOM, 등)이 발생할 경우 프로세스 자체가 종료될 수 있으며 이는 다른 컨슈머에 영향을 미칠 수 있다.
  → 컨슈머 스레드들이 비정상적으로 종료될 경우 데이터 처리에서 중복 또는 유실이 발생할 수 있다.
- 각 컨슈머 스레드 간에 영향이 미치지 않도록 스레드 세이프 로직, 변수를 적용해야 한다.

고려해야 할 점이 많지만, 멀티 스레드로 동작하는 멀티 컨슈머 스레드 애플리케이션을 안정적으로 지속 운영할 수 있도록 개발한다면 매우 효율적인 컨슈머를 운영할 수 있다.

컨슈머를 멀티 스레드로 활용하는 방식은 크게 두 가지로 나뉜다.

- 멀티 워커 스레드 전략 : 컨슈머 스레드는 1개만 실행하고 데이터 처리를 담당하는 워커 스레드(worker thread)를 여러 개 실행하는 방법
- 컨슈머 멀티 스레드 전략 : 컨슈머 인스턴스에서 poll() 메서드를 호출하는 스레드를 여러 개 띄워서 사용하는 방법

**카프카 컨슈머 멀티 워커 스레드 전략**

<img src="/img/4.3.1-2.png" width="1000px;">

멀티 스레드를 사용하면 각기 다른 레코드들의 데이터 처리를 동시에 실행할 수 있기 때문에 처리 시간을 현저히 줄일 수 있다.</br>
멀티 스레드를 생성하는 `ExecutorService` 자바 라이브러리를 사용하면 레코드를 병렬처리하는 스레드를 효율적으로 생성하고 관리할 수 있다.</br>

<img src="/img/4.3.1-3.png" width="1000px;">

**카프카 컨슈머 멀티 스레드 전략**

하나의 파티션은 동일 컨슈머 중 최대 1개까지 할당된다. 그리고 하나의 컨슈머는 여러 파티션에 할당될 수 있다.</br>
이런 특징을 이용하는 방법은 1개의 애플리케이션에 구독하고자 하는 토픽의 파티션 개수만큼 컨슈머 스레드 개수를 늘려서 운영하는 것이다.</br>
컨슈머 스레드를 늘려서 운영하면 각 스레드에 각 파티션이 할당되며, 파티션의 레코드들을 병렬처리할 수 있다.</br>

<img src="/img/4.3.1-4.png" width="1000px;">

주의해야 할 점은 구독하고자 하는 토픽의 파티션 개수만큼만 컨슈머 스레드를 운영해야 한다.</br>
컨슈머 스레드가 파티션 개수보다 많아지면 할당할 파티션 개수가 더는 없으므로 파티션에 할당되지 못한 컨슈머 스레드는 데이터 처리를 하지 않게 된다.</br>

### 컨슈머 랙

컨슈머 랙은 토픽의 최신 오프셋(LOG-END-OFFSET) 과 컨슈머 오프셋(CURRENT-OFFSET) 간의 차이다.</br>
컨슈무 랙은 컨슈머가 정상 동작하는지 여부를 확인할 수 있기 때문에 애플리케이션을 운영한다면 필수적으로 모니터링해야 하는 지표이다.</br>

<img src="/img/4.3.2-1.png" width="1000px;">

컨슈머 랙은 컨슈머 그룹과 토픽, 파티션별로 생성된다.</br>
1개의 토픽에 3개의 파티션이 있고 1개의 컨슈머 그룹이 토픽을 구독하여 데이터를 가져가면 컨슈머 랙은 총 3개가 된다.</br>

<img src="/img/4.3.2-2.png" width="1000px;">

프로듀서가 보내는 데이터양이 컨슈머의 데이터 처리량보다 크다면 컨슈머 랙은 늘어난다.</br>
반대로 프로듀서가 보내는 데이터 양이 컨슈머의 데이터 처리량보다 적으면 컨슈머 랙은 줄어들고 최솟값은 0으로 지연이 없음을 뜻한다.</br>

컨슈머 랙을 모니텅함으로써 컨슈머의 장애를 확인할 수 있고 파티션 개수를 정하는 데에 참고할 수 있다.

<img src="/img/4.3.2-5.png" width="1000px;">

<img src="/img/4.3.2-6.png" width="1000px;">

컨슈머 랙을 확인하는 방법은 3가지 이다.

- 카프카 명령어를 사용하는 방법
    - 단점
        - 일회성에 그치므로 지표를 지속적으로 기록하고 모니터링하기에는 부족하다

```bash
bin/kafka-consumer-groups.sh --bootstrap-server localhost:9200 \
--group my-group --describe
```

- 컨슈머 애플리케이션에서 metrics() 메서드를 사용하는 방법
    - 단점
        - 컨슈머가 정상 동작할 경우에만 확인 가능하다
        - 모든 컨슈머 애플리케이션에 컨슈머 랙 모니터링 코드를 중복해서 작성해야 한다.
        - 컨슈머 랙을 모니터링 하는 코드를 추가할 수 없는 서드 파티(third party) 애플리케이션의 컨슈머 랙 모니터링이 불가능하다.
- 외부 모니터링 툴을 사용하는 방법
    - Datadog, Confluent Control Center 와 같은 카프카 클러스터 종합 모니터링 툴을 사용하는 방식과 컨슈머 랙 모니터링만을 위한 툴인 Burrow 을 사용할 수 있다.
    - 모니터링 툴들은 클러스터와 연동되어 컨슈머의 데이터 처리와 별개로 지표를 수집하기 때문에 데이터를 활용하는 프로듀서나 컨슈머의 동작에 영향을 미치지 않는다.

**컨슈머 랙 모니터링 아키택처**

Burrow 를 통해 컨슈머 랙을 모니터링할 때는 이미 지나간 컨슈머 랙을 개별적으로 모니터링하기 위해서는 별개의 저장소와 대시보드를 사용하는 것이 효과적이다.

<img src="/img/4.3.2.1-5.png" width="1000px;">
